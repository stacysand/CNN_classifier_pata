{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35516713-5ede-4128-a1ff-bd0943ca3814",
   "metadata": {},
   "source": [
    "### Train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f7a90e8-da08-414c-a7e4-088314c7df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54bc6a7a-2a26-43fa-ab0c-ee7ce97bb995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    800\n",
      "1    800\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>audio_name</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50_block1_30_30_TA_7</td>\n",
       "      <td>50_block1_30_30_TA_7.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/50/50_block1_...</td>\n",
       "      <td>50_block1_30_30_TA_7.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/50_block1_3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50_block1_32_26_TA_9</td>\n",
       "      <td>50_block1_32_26_TA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/50/50_block1_...</td>\n",
       "      <td>50_block1_32_26_TA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/50_block1_3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50_block1_39_61_TA_8</td>\n",
       "      <td>50_block1_39_61_TA_8.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/50/50_block1_...</td>\n",
       "      <td>50_block1_39_61_TA_8.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/50_block1_3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50_block2_32_103_PA_9</td>\n",
       "      <td>50_block2_32_103_PA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/50/50_block2_...</td>\n",
       "      <td>50_block2_32_103_PA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/50_block2_3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50_block1_24_40_TA_9</td>\n",
       "      <td>50_block1_24_40_TA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/50/50_block1_...</td>\n",
       "      <td>50_block1_24_40_TA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/50_block1_2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>25_block2_39_106_PA_8</td>\n",
       "      <td>25_block2_39_106_PA_8.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/25/25_block2_...</td>\n",
       "      <td>25_block2_39_106_PA_8.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/25_block2_3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>25_block1_11_2_PA_3</td>\n",
       "      <td>25_block1_11_2_PA_3.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/25/25_block1_...</td>\n",
       "      <td>25_block1_11_2_PA_3.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/25_block1_1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>25_block1_47_31_TA_8</td>\n",
       "      <td>25_block1_47_31_TA_8.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/25/25_block1_...</td>\n",
       "      <td>25_block1_47_31_TA_8.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/25_block1_4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>25_block1_72_35_TA_9</td>\n",
       "      <td>25_block1_72_35_TA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/25/25_block1_...</td>\n",
       "      <td>25_block1_72_35_TA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/25_block1_7...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>25_block2_24_120_PA_9</td>\n",
       "      <td>25_block2_24_120_PA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/25/25_block2_...</td>\n",
       "      <td>25_block2_24_120_PA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/25_block2_2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name                 audio_name  \\\n",
       "0      50_block1_30_30_TA_7   50_block1_30_30_TA_7.wav   \n",
       "1      50_block1_32_26_TA_9   50_block1_32_26_TA_9.wav   \n",
       "2      50_block1_39_61_TA_8   50_block1_39_61_TA_8.wav   \n",
       "3     50_block2_32_103_PA_9  50_block2_32_103_PA_9.wav   \n",
       "4      50_block1_24_40_TA_9   50_block1_24_40_TA_9.wav   \n",
       "...                     ...                        ...   \n",
       "1595  25_block2_39_106_PA_8  25_block2_39_106_PA_8.wav   \n",
       "1596    25_block1_11_2_PA_3    25_block1_11_2_PA_3.wav   \n",
       "1597   25_block1_47_31_TA_8   25_block1_47_31_TA_8.wav   \n",
       "1598   25_block1_72_35_TA_9   25_block1_72_35_TA_9.wav   \n",
       "1599  25_block2_24_120_PA_9  25_block2_24_120_PA_9.wav   \n",
       "\n",
       "                                             audio_path  \\\n",
       "0     /Users/hela/Code/pata/Audio_data/50/50_block1_...   \n",
       "1     /Users/hela/Code/pata/Audio_data/50/50_block1_...   \n",
       "2     /Users/hela/Code/pata/Audio_data/50/50_block1_...   \n",
       "3     /Users/hela/Code/pata/Audio_data/50/50_block2_...   \n",
       "4     /Users/hela/Code/pata/Audio_data/50/50_block1_...   \n",
       "...                                                 ...   \n",
       "1595  /Users/hela/Code/pata/Audio_data/25/25_block2_...   \n",
       "1596  /Users/hela/Code/pata/Audio_data/25/25_block1_...   \n",
       "1597  /Users/hela/Code/pata/Audio_data/25/25_block1_...   \n",
       "1598  /Users/hela/Code/pata/Audio_data/25/25_block1_...   \n",
       "1599  /Users/hela/Code/pata/Audio_data/25/25_block2_...   \n",
       "\n",
       "                     image_name  \\\n",
       "0      50_block1_30_30_TA_7.png   \n",
       "1      50_block1_32_26_TA_9.png   \n",
       "2      50_block1_39_61_TA_8.png   \n",
       "3     50_block2_32_103_PA_9.png   \n",
       "4      50_block1_24_40_TA_9.png   \n",
       "...                         ...   \n",
       "1595  25_block2_39_106_PA_8.png   \n",
       "1596    25_block1_11_2_PA_3.png   \n",
       "1597   25_block1_47_31_TA_8.png   \n",
       "1598   25_block1_72_35_TA_9.png   \n",
       "1599  25_block2_24_120_PA_9.png   \n",
       "\n",
       "                                             image_path  label  \n",
       "0     /Users/hela/Code/pata/spectrograms/50_block1_3...      0  \n",
       "1     /Users/hela/Code/pata/spectrograms/50_block1_3...      0  \n",
       "2     /Users/hela/Code/pata/spectrograms/50_block1_3...      1  \n",
       "3     /Users/hela/Code/pata/spectrograms/50_block2_3...      0  \n",
       "4     /Users/hela/Code/pata/spectrograms/50_block1_2...      1  \n",
       "...                                                 ...    ...  \n",
       "1595  /Users/hela/Code/pata/spectrograms/25_block2_3...      0  \n",
       "1596  /Users/hela/Code/pata/spectrograms/25_block1_1...      0  \n",
       "1597  /Users/hela/Code/pata/spectrograms/25_block1_4...      1  \n",
       "1598  /Users/hela/Code/pata/spectrograms/25_block1_7...      1  \n",
       "1599  /Users/hela/Code/pata/spectrograms/25_block2_2...      0  \n",
       "\n",
       "[1600 rows x 6 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv('/Users/hela/Code/pata/data_labeled.csv')\n",
    "# df = df[['image_path', 'label']]\n",
    "print(df['label'].value_counts())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69dbffd8-3ebf-4b0d-bd93-285d260cde05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create label mappings\n",
    "# These mappings convert string → int (label2id) and allow reverse. Required by Hugging Face Trainer and model config.\n",
    "label2id = {\"0\": 0, \"1\": 1}\n",
    "id2label = {0: \"0\", 1: \"1\"}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6ad4e25-f890-40ed-b9b6-1f707dcb0926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>audio_name</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>01_block1_78_29_PA_7</td>\n",
       "      <td>01_block1_78_29_PA_7.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/01/01_block1_...</td>\n",
       "      <td>01_block1_78_29_PA_7.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/01_block1_7...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>08_block1_5_25_TA_6</td>\n",
       "      <td>08_block1_5_25_TA_6.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/08/08_block1_...</td>\n",
       "      <td>08_block1_5_25_TA_6.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/08_block1_5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>41_block1_1_25_PA_1</td>\n",
       "      <td>41_block1_1_25_PA_1.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/41/41_block1_...</td>\n",
       "      <td>41_block1_1_25_PA_1.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/41_block1_1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>41_block2_72_84_PA_9</td>\n",
       "      <td>41_block2_72_84_PA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/41/41_block2_...</td>\n",
       "      <td>41_block2_72_84_PA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/41_block2_7...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>39_block2_16_84_PA_9</td>\n",
       "      <td>39_block2_16_84_PA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/39/39_block2_...</td>\n",
       "      <td>39_block2_16_84_PA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/39_block2_1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>15_block1_47_55_TA_8</td>\n",
       "      <td>15_block1_47_55_TA_8.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/15/15_block1_...</td>\n",
       "      <td>15_block1_47_55_TA_8.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/15_block1_4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>49_block2_2_141_PA_2</td>\n",
       "      <td>49_block2_2_141_PA_2.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/49/49_block2_...</td>\n",
       "      <td>49_block2_2_141_PA_2.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/49_block2_2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>33_block1_5_56_TA_6</td>\n",
       "      <td>33_block1_5_56_TA_6.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/33/33_block1_...</td>\n",
       "      <td>33_block1_5_56_TA_6.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/33_block1_5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>40_block1_28_55_PA_4</td>\n",
       "      <td>40_block1_28_55_PA_4.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/40/40_block1_...</td>\n",
       "      <td>40_block1_28_55_PA_4.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/40_block1_2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>02_block1_39_50_PA_8</td>\n",
       "      <td>02_block1_39_50_PA_8.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/02/02_block1_...</td>\n",
       "      <td>02_block1_39_50_PA_8.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/02_block1_3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                audio_name  \\\n",
       "1066  01_block1_78_29_PA_7  01_block1_78_29_PA_7.wav   \n",
       "1038   08_block1_5_25_TA_6   08_block1_5_25_TA_6.wav   \n",
       "1221   41_block1_1_25_PA_1   41_block1_1_25_PA_1.wav   \n",
       "1239  41_block2_72_84_PA_9  41_block2_72_84_PA_9.wav   \n",
       "1134  39_block2_16_84_PA_9  39_block2_16_84_PA_9.wav   \n",
       "...                    ...                       ...   \n",
       "1338  15_block1_47_55_TA_8  15_block1_47_55_TA_8.wav   \n",
       "1399  49_block2_2_141_PA_2  49_block2_2_141_PA_2.wav   \n",
       "242    33_block1_5_56_TA_6   33_block1_5_56_TA_6.wav   \n",
       "1438  40_block1_28_55_PA_4  40_block1_28_55_PA_4.wav   \n",
       "314   02_block1_39_50_PA_8  02_block1_39_50_PA_8.wav   \n",
       "\n",
       "                                             audio_path  \\\n",
       "1066  /Users/hela/Code/pata/Audio_data/01/01_block1_...   \n",
       "1038  /Users/hela/Code/pata/Audio_data/08/08_block1_...   \n",
       "1221  /Users/hela/Code/pata/Audio_data/41/41_block1_...   \n",
       "1239  /Users/hela/Code/pata/Audio_data/41/41_block2_...   \n",
       "1134  /Users/hela/Code/pata/Audio_data/39/39_block2_...   \n",
       "...                                                 ...   \n",
       "1338  /Users/hela/Code/pata/Audio_data/15/15_block1_...   \n",
       "1399  /Users/hela/Code/pata/Audio_data/49/49_block2_...   \n",
       "242   /Users/hela/Code/pata/Audio_data/33/33_block1_...   \n",
       "1438  /Users/hela/Code/pata/Audio_data/40/40_block1_...   \n",
       "314   /Users/hela/Code/pata/Audio_data/02/02_block1_...   \n",
       "\n",
       "                    image_name  \\\n",
       "1066  01_block1_78_29_PA_7.png   \n",
       "1038   08_block1_5_25_TA_6.png   \n",
       "1221   41_block1_1_25_PA_1.png   \n",
       "1239  41_block2_72_84_PA_9.png   \n",
       "1134  39_block2_16_84_PA_9.png   \n",
       "...                        ...   \n",
       "1338  15_block1_47_55_TA_8.png   \n",
       "1399  49_block2_2_141_PA_2.png   \n",
       "242    33_block1_5_56_TA_6.png   \n",
       "1438  40_block1_28_55_PA_4.png   \n",
       "314   02_block1_39_50_PA_8.png   \n",
       "\n",
       "                                             image_path  label  \n",
       "1066  /Users/hela/Code/pata/spectrograms/01_block1_7...      0  \n",
       "1038  /Users/hela/Code/pata/spectrograms/08_block1_5...      1  \n",
       "1221  /Users/hela/Code/pata/spectrograms/41_block1_1...      0  \n",
       "1239  /Users/hela/Code/pata/spectrograms/41_block2_7...      0  \n",
       "1134  /Users/hela/Code/pata/spectrograms/39_block2_1...      1  \n",
       "...                                                 ...    ...  \n",
       "1338  /Users/hela/Code/pata/spectrograms/15_block1_4...      1  \n",
       "1399  /Users/hela/Code/pata/spectrograms/49_block2_2...      0  \n",
       "242   /Users/hela/Code/pata/spectrograms/33_block1_5...      1  \n",
       "1438  /Users/hela/Code/pata/spectrograms/40_block1_2...      0  \n",
       "314   /Users/hela/Code/pata/spectrograms/02_block1_3...      0  \n",
       "\n",
       "[1280 rows x 6 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84a94179-e152-42ab-89cb-8aeb0590de44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0338616ffe95437d91c5aa929954129e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e6aadb1ad2425d9ab84f4ef2184370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['name', 'audio_name', 'audio_path', 'image_name', 'image_path', 'label', '__index_level_0__', 'image'],\n",
       "    num_rows: 1280\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define image loading function\n",
    "# Fn opens image by its path, converts to RGB, adds actual PIL.Image object to the example\n",
    "\n",
    "def load_image(example):\n",
    "    example['image'] = Image.open(example['image_path']).convert('RGB')\n",
    "    return example\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(load_image)  # apply fn load_image() row-by-row\n",
    "test_dataset = Dataset.from_pandas(test_df).map(load_image)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "171fe512-e24c-41bf-9bc4-d3d4fc7d6579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['name', 'audio_name', 'audio_path', 'image_name', 'image_path', 'label', '__index_level_0__', 'image'],\n",
       "        num_rows: 1280\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['name', 'audio_name', 'audio_path', 'image_name', 'image_path', 'label', '__index_level_0__', 'image'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create datasetDict\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed49108-a46c-4ec2-a397-f944335a9c34",
   "metadata": {},
   "source": [
    "### Image processor and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "827179b8-8fab-4859-bff1-60545d1164c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ResNetForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b41806b-9e88-4bd8-9562-9538f5049164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it was problamatic to download directly via transformers, so I downloaded on the computer manually:\n",
    "# uv pip install huggingface_hub\n",
    "# huggingface-cli download microsoft/resnet-50 OR huggingface-cli download microsoft/resnet-50 --local-dir ./resnet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cdc917c7-d04c-4121-8e93-8d8083ee49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image processor\n",
    "image_processor = AutoImageProcessor.from_pretrained('/Users/hela/Code/pata/resnet-50/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f3745c37-540a-4e37-8eba-88740654fa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at /Users/hela/Code/pata/resnet-50/ and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model with 3 classes\n",
    "model = ResNetForImageClassification.from_pretrained(\n",
    "    '/Users/hela/Code/pata/resnet-50/',\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True  # replaces the classification head\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1a939704-3c0d-4d80-84b2-ce16396ec88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 4,098 / 23,512,130 (0.02%)\n"
     ]
    }
   ],
   "source": [
    "# Freeze backbone (only train head):\n",
    "\n",
    "# Freeze all layers except the classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name:  # Freeze everything except classifier\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True  # Keep classifier trainable\n",
    "\n",
    "# Verify\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fe237-b81a-4c7c-8519-c5f789cfed7d",
   "metadata": {},
   "source": [
    "### Data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41d96b27-d890-42a0-b4ac-0537e5eac16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04a615bc-e185-4c61-b41a-9b208ce9806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for training data\n",
    "# details: https://docs.pytorch.org/vision/main/transforms.html#v1-or-v2-which-one-should-i-use\n",
    "# I don't need resize or croping because images are already 224*224 and because of the task specifisity\n",
    "train_transforms = transforms.Compose([  # chain multiple image transforms together into one pipeline\n",
    "    transforms.ToTensor(),  # PIL Image (H×W×C, values 0-255) → PyTorch tensor (C×H×W, values 0.0-1.0)\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)  # apply (pixel - mean) / std for each channel using ImageNet statistics (to center px vals around 0 with sd=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41972259-ddf7-4579-ab57-67f7300881a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for testing data\n",
    "test_transforms = transforms.Compose([  # chain multiple image transforms together into one pipeline\n",
    "    transforms.ToTensor(),  # convert PIL into PyTorch tensor, change color channels from H×W×C to C×H×W, scale px vals from [0, 255] to [0.0, 1.0]\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)  # center px vals around 0 with sd=1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9dbe2a15-fc52-4252-82d6-9e2196a1e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing fn\n",
    "# take a batch of examples, for each example, apply transforms, collect results in a list, return a modified batch\n",
    "def preprocess_train(examples):\n",
    "    images = [Image.open(path).convert('RGB') for path in examples['image_path']]\n",
    "    examples['pixel_values'] = [train_transforms(image) for image in images]\n",
    "    return examples\n",
    "\n",
    "def preprocess_test(examples):\n",
    "    images = [Image.open(path).convert('RGB') for path in examples['image_path']]\n",
    "    examples['pixel_values'] = [test_transforms(image) for image in images]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "776b7490-0ab3-48fd-9d3d-696c005fd19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70912f1da48c47f08d9ae4e6f17ced56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0875f85c114633ad368b9ea0439ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply transforms to datasets 'on-the-fly'\n",
    "#train_dataset.set_transform(preprocess_train)\n",
    "#test_dataset.set_transform(preprocess_test)\n",
    "\n",
    "# Apply transforms to datasets before training\n",
    "# Remove only the columns we don't need, but KEEP 'label\n",
    "columns_to_remove = [col for col in train_dataset.column_names if col !='label']\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_train, batched=True, remove_columns=columns_to_remove)\n",
    "test_dataset = test_dataset.map(preprocess_test, batched=True, remove_columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c71873-9336-4ab4-992b-88685661903c",
   "metadata": {},
   "source": [
    "### Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0aa5487-6e8c-4aee-901e-c196f677f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c4f5b2a-08d6-4620-af6e-82bcfab6e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collator simply stacks individual examples into batches\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a7fec-71bc-4c2b-8643-ecfd8b4b9967",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "729dce01-d0c4-4384-9266-53b2ff4e63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "da580b44-317f-44db-aae8-58e5518215fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred  # eval_pred contains model logits and true labels\n",
    "    predictions = np.argmax(predictions, axis=1)  # np.argmax selects the class with highest score\n",
    "    return metric.compute(predictions=predictions, references=labels)  # return {'accuracy': 0.XX}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fe096-4d1c-42d0-bd0c-ac1cb79b7332",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "843665f2-8e95-4f34-b5c3-4da1dfebf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb1ac5-0f4a-4d57-8d1f-8bc50c2d45f8",
   "metadata": {},
   "source": [
    "total gradient updates = epochs × (dataset_size / batch_size)\n",
    "\n",
    "- lr\n",
    "\n",
    "lr = 1e-2=0.01 / 1e-3=0.001 / 1e-4=0.0001 / 1e-5=0.00001 (might also be 5e-5=0.00005 / 8e-5=0.00008 / 3e-4=0.0003)\n",
    "Automatic tool for lr (PyTorch Lightning): Tuner(lr_find=True)\n",
    "(start from big (e.g.,1e-2=0.01) and adjust downward. If the cost oscillates or increases in the first few epochs, reduce lr by 10)\n",
    "\n",
    "- learning rate scheduling\n",
    "\n",
    "lrst = \"linear\" / \"cosine\" / \"cosine_with_restarts\" / \"polynomial\" / \"constant\" / \"constant_with_warmup\"  \n",
    "wr = 0.1 = 10% of training steps will be warmup (LR gradually increases) OR use warmup_steps=100 for exact number of steps  \n",
    "**Cosine with Warmup:** lrst=\"cosine\", wr=0.1 (LR increases linearly for 10% of training (warmup), Then decreases following a cosine curve to 0. Best for: Fine-tuning pre-trained models.)  \n",
    "**Cosine with Restarts:** lrst=\"cosine_with_restarts\", wr=0.1 (Periodically \"restarts\" the LR back to a higher value. Can help escape local minima. Best for: Longer training runs or when stuck in plateaus)  \n",
    "**Linear Decay with Warmup:** lrst=\"linear\", wr=0.05 (LR increases linearly for 5% of training, then decreases linearly to 0)  \n",
    "**Constant with Warmup:** lrst=\"constant_with_warmup\", warmup_steps=100 (LR increases during warmup (for first 100 steps), then stays constant. Best for:** When you've already tuned your LR and it works well)  \n",
    "\n",
    "\n",
    "- bs\n",
    "\n",
    "bs = (rule b**2) 8→16→32→64→128→256 (find the one that fits my GPU/CPU)\n",
    "for fine-tuning small sizes recommended, and smaller sizes work better with small datasets\n",
    "(start with small (e.g., 8) and gradually increase to find the sweet spot between training speed and convergence\n",
    "\n",
    "- epochs\n",
    "\n",
    "epochs = 4-10? 10-15? 20-40? 50?\n",
    "no need to manually tune epochs - monitor validation acc and stop when it doesn't improve for a number of epochs\n",
    "Strategies are random:\n",
    "Starting point: 20-50 epochs\n",
    "Early stopping with patience of 10-15 epochs\n",
    "With around 1000 images per class for binary classification, 10 epochs allow to reach 99% validation acc using ResNet50\n",
    "4 to run quickly\n",
    "\n",
    "- wd\n",
    "\n",
    "wd = 5e-2=0.05 / 1e-2=0.01 / 1e-3=0.001 / 1e-4=0.0001 / 1e-5=0.00001\n",
    "(start with small values (e.g., 1e-4 or 1e-3) and increase if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a1793f00-87f0-49a3-8809-522d38427bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "lrst = \"cosine\"\n",
    "wr = 0.15\n",
    "bs = 16\n",
    "epochs = 10\n",
    "wd = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d445c05a-0d88-4557-bdc8-08930a4c3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resnet-50-finetuned\",\n",
    "    remove_unused_columns=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    #fp16=False,  # Disable FP16 to use fp32\n",
    "    #bf16=False,  # Disable BF16 to use fp32\n",
    "    learning_rate=lr,  # lr\n",
    "    per_device_train_batch_size=bs, # bs\n",
    "    per_device_eval_batch_size=bs*2, # bs\n",
    "    num_train_epochs=epochs, # epochs\n",
    "    weight_decay=wd, # wd\n",
    "    lr_scheduler_type=lrst,  # lrst\n",
    "    warmup_ratio=wr,  # wr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42cf18-20be-400f-af41-d08bc8e4ca26",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "33c4477e-eece-4ffa-aa27-8671f56da1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3dc0d1a8-2f7a-4e90-841a-592acd8a4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    processing_class=image_processor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142db8c-6fc0-46fe-b17d-42319eef5eeb",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b767d7a4-2e4b-4f1d-a29e-2b65eddbbd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='671' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [671/800 10:33 < 02:02, 1.06 it/s, Epoch 8.38/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.589250</td>\n",
       "      <td>0.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.574246</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.566992</td>\n",
       "      <td>0.731250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.567856</td>\n",
       "      <td>0.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.589298</td>\n",
       "      <td>0.728125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.568902</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.347200</td>\n",
       "      <td>0.562363</td>\n",
       "      <td>0.728125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.347200</td>\n",
       "      <td>0.590542</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Runs N epochs of training\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Updates model weights via AdamW optimizer (default)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Evaluates accuracy after each epoch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Saves checkpoints to ./resnet-50-finetuned/\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Result: A fine-tuned model with saved weights and training logs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/transformers/trainer.py:2679\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2674\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Runs N epochs of training\n",
    "# Updates model weights via AdamW optimizer (default)\n",
    "# Evaluates accuracy after each epoch\n",
    "# Saves checkpoints to ./resnet-50-finetuned/\n",
    "# Result: A fine-tuned model with saved weights and training logs\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99a74d-9ee8-4051-bdd8-5262a2959903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
