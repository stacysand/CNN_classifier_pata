{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35516713-5ede-4128-a1ff-bd0943ca3814",
   "metadata": {},
   "source": [
    "### Train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7a90e8-da08-414c-a7e4-088314c7df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54bc6a7a-2a26-43fa-ab0c-ee7ce97bb995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    800\n",
      "1    800\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>audio_name</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50_block1_30_30_TA_7</td>\n",
       "      <td>50_block1_30_30_TA_7.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/50/50_block1_...</td>\n",
       "      <td>50_block1_30_30_TA_7.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/50_block1_3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50_block1_32_26_TA_9</td>\n",
       "      <td>50_block1_32_26_TA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/50/50_block1_...</td>\n",
       "      <td>50_block1_32_26_TA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/50_block1_3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50_block1_39_61_TA_8</td>\n",
       "      <td>50_block1_39_61_TA_8.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/50/50_block1_...</td>\n",
       "      <td>50_block1_39_61_TA_8.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/50_block1_3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50_block2_32_103_PA_9</td>\n",
       "      <td>50_block2_32_103_PA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/50/50_block2_...</td>\n",
       "      <td>50_block2_32_103_PA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/50_block2_3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50_block1_24_40_TA_9</td>\n",
       "      <td>50_block1_24_40_TA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/50/50_block1_...</td>\n",
       "      <td>50_block1_24_40_TA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/50_block1_2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>25_block2_39_106_PA_8</td>\n",
       "      <td>25_block2_39_106_PA_8.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/25/25_block2_...</td>\n",
       "      <td>25_block2_39_106_PA_8.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/25_block2_3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>25_block1_11_2_PA_3</td>\n",
       "      <td>25_block1_11_2_PA_3.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/25/25_block1_...</td>\n",
       "      <td>25_block1_11_2_PA_3.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/25_block1_1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>25_block1_47_31_TA_8</td>\n",
       "      <td>25_block1_47_31_TA_8.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/25/25_block1_...</td>\n",
       "      <td>25_block1_47_31_TA_8.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/25_block1_4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>25_block1_72_35_TA_9</td>\n",
       "      <td>25_block1_72_35_TA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/25/25_block1_...</td>\n",
       "      <td>25_block1_72_35_TA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/25_block1_7...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>25_block2_24_120_PA_9</td>\n",
       "      <td>25_block2_24_120_PA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/25/25_block2_...</td>\n",
       "      <td>25_block2_24_120_PA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/25_block2_2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name                 audio_name  \\\n",
       "0      50_block1_30_30_TA_7   50_block1_30_30_TA_7.wav   \n",
       "1      50_block1_32_26_TA_9   50_block1_32_26_TA_9.wav   \n",
       "2      50_block1_39_61_TA_8   50_block1_39_61_TA_8.wav   \n",
       "3     50_block2_32_103_PA_9  50_block2_32_103_PA_9.wav   \n",
       "4      50_block1_24_40_TA_9   50_block1_24_40_TA_9.wav   \n",
       "...                     ...                        ...   \n",
       "1595  25_block2_39_106_PA_8  25_block2_39_106_PA_8.wav   \n",
       "1596    25_block1_11_2_PA_3    25_block1_11_2_PA_3.wav   \n",
       "1597   25_block1_47_31_TA_8   25_block1_47_31_TA_8.wav   \n",
       "1598   25_block1_72_35_TA_9   25_block1_72_35_TA_9.wav   \n",
       "1599  25_block2_24_120_PA_9  25_block2_24_120_PA_9.wav   \n",
       "\n",
       "                                             audio_path  \\\n",
       "0     /Users/hela/Code/pata/Audio_data/50/50_block1_...   \n",
       "1     /Users/hela/Code/pata/Audio_data/50/50_block1_...   \n",
       "2     /Users/hela/Code/pata/Audio_data/50/50_block1_...   \n",
       "3     /Users/hela/Code/pata/Audio_data/50/50_block2_...   \n",
       "4     /Users/hela/Code/pata/Audio_data/50/50_block1_...   \n",
       "...                                                 ...   \n",
       "1595  /Users/hela/Code/pata/Audio_data/25/25_block2_...   \n",
       "1596  /Users/hela/Code/pata/Audio_data/25/25_block1_...   \n",
       "1597  /Users/hela/Code/pata/Audio_data/25/25_block1_...   \n",
       "1598  /Users/hela/Code/pata/Audio_data/25/25_block1_...   \n",
       "1599  /Users/hela/Code/pata/Audio_data/25/25_block2_...   \n",
       "\n",
       "                     image_name  \\\n",
       "0      50_block1_30_30_TA_7.png   \n",
       "1      50_block1_32_26_TA_9.png   \n",
       "2      50_block1_39_61_TA_8.png   \n",
       "3     50_block2_32_103_PA_9.png   \n",
       "4      50_block1_24_40_TA_9.png   \n",
       "...                         ...   \n",
       "1595  25_block2_39_106_PA_8.png   \n",
       "1596    25_block1_11_2_PA_3.png   \n",
       "1597   25_block1_47_31_TA_8.png   \n",
       "1598   25_block1_72_35_TA_9.png   \n",
       "1599  25_block2_24_120_PA_9.png   \n",
       "\n",
       "                                             image_path  label  \n",
       "0     /Users/hela/Code/pata/spectrograms/50_block1_3...      0  \n",
       "1     /Users/hela/Code/pata/spectrograms/50_block1_3...      0  \n",
       "2     /Users/hela/Code/pata/spectrograms/50_block1_3...      1  \n",
       "3     /Users/hela/Code/pata/spectrograms/50_block2_3...      0  \n",
       "4     /Users/hela/Code/pata/spectrograms/50_block1_2...      1  \n",
       "...                                                 ...    ...  \n",
       "1595  /Users/hela/Code/pata/spectrograms/25_block2_3...      0  \n",
       "1596  /Users/hela/Code/pata/spectrograms/25_block1_1...      0  \n",
       "1597  /Users/hela/Code/pata/spectrograms/25_block1_4...      1  \n",
       "1598  /Users/hela/Code/pata/spectrograms/25_block1_7...      1  \n",
       "1599  /Users/hela/Code/pata/spectrograms/25_block2_2...      0  \n",
       "\n",
       "[1600 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv('/Users/hela/Code/pata/data_labeled.csv')\n",
    "# df = df[['image_path', 'label']]\n",
    "print(df['label'].value_counts())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69dbffd8-3ebf-4b0d-bd93-285d260cde05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create label mappings\n",
    "# These mappings convert string → int (label2id) and allow reverse. Required by Hugging Face Trainer and model config.\n",
    "label2id = {\"0\": 0, \"1\": 1}\n",
    "id2label = {0: \"0\", 1: \"1\"}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ad4e25-f890-40ed-b9b6-1f707dcb0926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>audio_name</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>01_block1_78_29_PA_7</td>\n",
       "      <td>01_block1_78_29_PA_7.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/01/01_block1_...</td>\n",
       "      <td>01_block1_78_29_PA_7.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/01_block1_7...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>08_block1_5_25_TA_6</td>\n",
       "      <td>08_block1_5_25_TA_6.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/08/08_block1_...</td>\n",
       "      <td>08_block1_5_25_TA_6.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/08_block1_5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>41_block1_1_25_PA_1</td>\n",
       "      <td>41_block1_1_25_PA_1.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/41/41_block1_...</td>\n",
       "      <td>41_block1_1_25_PA_1.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/41_block1_1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>41_block2_72_84_PA_9</td>\n",
       "      <td>41_block2_72_84_PA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/41/41_block2_...</td>\n",
       "      <td>41_block2_72_84_PA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/41_block2_7...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>39_block2_16_84_PA_9</td>\n",
       "      <td>39_block2_16_84_PA_9.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/39/39_block2_...</td>\n",
       "      <td>39_block2_16_84_PA_9.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/39_block2_1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>15_block1_47_55_TA_8</td>\n",
       "      <td>15_block1_47_55_TA_8.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/15/15_block1_...</td>\n",
       "      <td>15_block1_47_55_TA_8.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/15_block1_4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>49_block2_2_141_PA_2</td>\n",
       "      <td>49_block2_2_141_PA_2.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/49/49_block2_...</td>\n",
       "      <td>49_block2_2_141_PA_2.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/49_block2_2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>33_block1_5_56_TA_6</td>\n",
       "      <td>33_block1_5_56_TA_6.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/33/33_block1_...</td>\n",
       "      <td>33_block1_5_56_TA_6.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/33_block1_5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>40_block1_28_55_PA_4</td>\n",
       "      <td>40_block1_28_55_PA_4.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/40/40_block1_...</td>\n",
       "      <td>40_block1_28_55_PA_4.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/40_block1_2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>02_block1_39_50_PA_8</td>\n",
       "      <td>02_block1_39_50_PA_8.wav</td>\n",
       "      <td>/Users/hela/Code/pata/Audio_data/02/02_block1_...</td>\n",
       "      <td>02_block1_39_50_PA_8.png</td>\n",
       "      <td>/Users/hela/Code/pata/spectrograms/02_block1_3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                audio_name  \\\n",
       "1066  01_block1_78_29_PA_7  01_block1_78_29_PA_7.wav   \n",
       "1038   08_block1_5_25_TA_6   08_block1_5_25_TA_6.wav   \n",
       "1221   41_block1_1_25_PA_1   41_block1_1_25_PA_1.wav   \n",
       "1239  41_block2_72_84_PA_9  41_block2_72_84_PA_9.wav   \n",
       "1134  39_block2_16_84_PA_9  39_block2_16_84_PA_9.wav   \n",
       "...                    ...                       ...   \n",
       "1338  15_block1_47_55_TA_8  15_block1_47_55_TA_8.wav   \n",
       "1399  49_block2_2_141_PA_2  49_block2_2_141_PA_2.wav   \n",
       "242    33_block1_5_56_TA_6   33_block1_5_56_TA_6.wav   \n",
       "1438  40_block1_28_55_PA_4  40_block1_28_55_PA_4.wav   \n",
       "314   02_block1_39_50_PA_8  02_block1_39_50_PA_8.wav   \n",
       "\n",
       "                                             audio_path  \\\n",
       "1066  /Users/hela/Code/pata/Audio_data/01/01_block1_...   \n",
       "1038  /Users/hela/Code/pata/Audio_data/08/08_block1_...   \n",
       "1221  /Users/hela/Code/pata/Audio_data/41/41_block1_...   \n",
       "1239  /Users/hela/Code/pata/Audio_data/41/41_block2_...   \n",
       "1134  /Users/hela/Code/pata/Audio_data/39/39_block2_...   \n",
       "...                                                 ...   \n",
       "1338  /Users/hela/Code/pata/Audio_data/15/15_block1_...   \n",
       "1399  /Users/hela/Code/pata/Audio_data/49/49_block2_...   \n",
       "242   /Users/hela/Code/pata/Audio_data/33/33_block1_...   \n",
       "1438  /Users/hela/Code/pata/Audio_data/40/40_block1_...   \n",
       "314   /Users/hela/Code/pata/Audio_data/02/02_block1_...   \n",
       "\n",
       "                    image_name  \\\n",
       "1066  01_block1_78_29_PA_7.png   \n",
       "1038   08_block1_5_25_TA_6.png   \n",
       "1221   41_block1_1_25_PA_1.png   \n",
       "1239  41_block2_72_84_PA_9.png   \n",
       "1134  39_block2_16_84_PA_9.png   \n",
       "...                        ...   \n",
       "1338  15_block1_47_55_TA_8.png   \n",
       "1399  49_block2_2_141_PA_2.png   \n",
       "242    33_block1_5_56_TA_6.png   \n",
       "1438  40_block1_28_55_PA_4.png   \n",
       "314   02_block1_39_50_PA_8.png   \n",
       "\n",
       "                                             image_path  label  \n",
       "1066  /Users/hela/Code/pata/spectrograms/01_block1_7...      0  \n",
       "1038  /Users/hela/Code/pata/spectrograms/08_block1_5...      1  \n",
       "1221  /Users/hela/Code/pata/spectrograms/41_block1_1...      0  \n",
       "1239  /Users/hela/Code/pata/spectrograms/41_block2_7...      0  \n",
       "1134  /Users/hela/Code/pata/spectrograms/39_block2_1...      1  \n",
       "...                                                 ...    ...  \n",
       "1338  /Users/hela/Code/pata/spectrograms/15_block1_4...      1  \n",
       "1399  /Users/hela/Code/pata/spectrograms/49_block2_2...      0  \n",
       "242   /Users/hela/Code/pata/spectrograms/33_block1_5...      1  \n",
       "1438  /Users/hela/Code/pata/spectrograms/40_block1_2...      0  \n",
       "314   /Users/hela/Code/pata/spectrograms/02_block1_3...      0  \n",
       "\n",
       "[1280 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a94179-e152-42ab-89cb-8aeb0590de44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324994d6240949dfa66d16e6da00f49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bc4fe14b5f46b7a9db15de4757a99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['name', 'audio_name', 'audio_path', 'image_name', 'image_path', 'label', '__index_level_0__', 'image'],\n",
       "    num_rows: 1280\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define image loading function\n",
    "# Fn opens image by its path, converts to RGB, adds actual PIL.Image object to the example\n",
    "\n",
    "def load_image(example):\n",
    "    example['image'] = Image.open(example['image_path']).convert('RGB')\n",
    "    return example\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(load_image)  # apply fn load_image() row-by-row\n",
    "test_dataset = Dataset.from_pandas(test_df).map(load_image)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171fe512-e24c-41bf-9bc4-d3d4fc7d6579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['name', 'audio_name', 'audio_path', 'image_name', 'image_path', 'label', '__index_level_0__', 'image'],\n",
       "        num_rows: 1280\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['name', 'audio_name', 'audio_path', 'image_name', 'image_path', 'label', '__index_level_0__', 'image'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create datasetDict\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed49108-a46c-4ec2-a397-f944335a9c34",
   "metadata": {},
   "source": [
    "### Image processor and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "827179b8-8fab-4859-bff1-60545d1164c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ResNetForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b41806b-9e88-4bd8-9562-9538f5049164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it was problamatic to download directly via transformers, so I downloaded on the computer manually:\n",
    "# uv pip install huggingface_hub\n",
    "# huggingface-cli download microsoft/resnet-50 OR huggingface-cli download microsoft/resnet-50 --local-dir ./resnet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc917c7-d04c-4121-8e93-8d8083ee49cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load image processor\n",
    "image_processor = AutoImageProcessor.from_pretrained('/Users/hela/Code/pata/resnet-50/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3745c37-540a-4e37-8eba-88740654fa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at /Users/hela/Code/pata/resnet-50/ and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model with 3 classes\n",
    "model = ResNetForImageClassification.from_pretrained(\n",
    "    '/Users/hela/Code/pata/resnet-50/',\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True  # replaces the classification head\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a939704-3c0d-4d80-84b2-ce16396ec88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 4,098 / 23,512,130 (0.02%)\n"
     ]
    }
   ],
   "source": [
    "# Freeze backbone (only train head):\n",
    "\n",
    "# Freeze all layers except the classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name:  # Freeze everything except classifier\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True  # Keep classifier trainable\n",
    "\n",
    "# Verify\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fe237-b81a-4c7c-8519-c5f789cfed7d",
   "metadata": {},
   "source": [
    "### Data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d96b27-d890-42a0-b4ac-0537e5eac16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04a615bc-e185-4c61-b41a-9b208ce9806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for training data\n",
    "# details: https://docs.pytorch.org/vision/main/transforms.html#v1-or-v2-which-one-should-i-use\n",
    "# I don't need resize or croping because images are already 224*224 and because of the task specifisity\n",
    "train_transforms = transforms.Compose([  # chain multiple image transforms together into one pipeline\n",
    "    transforms.ToTensor(),  # PIL Image (H×W×C, values 0-255) → PyTorch tensor (C×H×W, values 0.0-1.0)\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)  # apply (pixel - mean) / std for each channel using ImageNet statistics (to center px vals around 0 with sd=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41972259-ddf7-4579-ab57-67f7300881a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for testing data\n",
    "test_transforms = transforms.Compose([  # chain multiple image transforms together into one pipeline\n",
    "    transforms.ToTensor(),  # convert PIL into PyTorch tensor, change color channels from H×W×C to C×H×W, scale px vals from [0, 255] to [0.0, 1.0]\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)  # center px vals around 0 with sd=1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dbe2a15-fc52-4252-82d6-9e2196a1e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing fn\n",
    "# take a batch of examples, for each example, apply transforms, collect results in a list, return a modified batch\n",
    "def preprocess_train(examples):\n",
    "    images = [Image.open(path).convert('RGB') for path in examples['image_path']]\n",
    "    examples['pixel_values'] = [train_transforms(image) for image in images]\n",
    "    return examples\n",
    "\n",
    "def preprocess_test(examples):\n",
    "    images = [Image.open(path).convert('RGB') for path in examples['image_path']]\n",
    "    examples['pixel_values'] = [test_transforms(image) for image in images]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "776b7490-0ab3-48fd-9d3d-696c005fd19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f79356410e441139be5dc94084ca40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2249f45e3cf846ef91056b3f31433f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply transforms to datasets 'on-the-fly'\n",
    "#train_dataset.set_transform(preprocess_train)\n",
    "#test_dataset.set_transform(preprocess_test)\n",
    "\n",
    "# Apply transforms to datasets before training\n",
    "# Remove only the columns we don't need, but KEEP 'label\n",
    "columns_to_remove = [col for col in train_dataset.column_names if col !='label']\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_train, batched=True, remove_columns=columns_to_remove)\n",
    "test_dataset = test_dataset.map(preprocess_test, batched=True, remove_columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c71873-9336-4ab4-992b-88685661903c",
   "metadata": {},
   "source": [
    "### Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0aa5487-6e8c-4aee-901e-c196f677f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c4f5b2a-08d6-4620-af6e-82bcfab6e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collator simply stacks individual examples into batches\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a7fec-71bc-4c2b-8643-ecfd8b4b9967",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729dce01-d0c4-4384-9266-53b2ff4e63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da580b44-317f-44db-aae8-58e5518215fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred  # eval_pred contains model logits and true labels\n",
    "    predictions = np.argmax(predictions, axis=1)  # np.argmax selects the class with highest score\n",
    "    return metric.compute(predictions=predictions, references=labels)  # return {'accuracy': 0.XX}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fe096-4d1c-42d0-bd0c-ac1cb79b7332",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "843665f2-8e95-4f34-b5c3-4da1dfebf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb1ac5-0f4a-4d57-8d1f-8bc50c2d45f8",
   "metadata": {},
   "source": [
    "total gradient updates = epochs × (dataset_size / batch_size)\n",
    "\n",
    "- lr\n",
    "\n",
    "lr = 1e-2=0.01 / 1e-3=0.001 / 1e-4=0.0001 / 1e-5=0.00001 (might also be 5e-5=0.00005 / 8e-5=0.00008 / 3e-4=0.0003)\n",
    "Automatic tool for lr (PyTorch Lightning): Tuner(lr_find=True)\n",
    "(start from big (e.g.,1e-2=0.01) and adjust downward. If the cost oscillates or increases in the first few epochs, reduce lr by 10)\n",
    "\n",
    "- learning rate scheduling\n",
    "\n",
    "lrst = \"linear\" / \"cosine\" / \"cosine_with_restarts\" / \"polynomial\" / \"constant\" / \"constant_with_warmup\"  \n",
    "wr = 0.1 = 10% of training steps will be warmup (LR gradually increases) OR use warmup_steps=100 for exact number of steps  \n",
    "**Cosine with Warmup:** lrst=\"cosine\", wr=0.1 (LR increases linearly for 10% of training (warmup), Then decreases following a cosine curve to 0. Best for: Fine-tuning pre-trained models.)  \n",
    "**Cosine with Restarts:** lrst=\"cosine_with_restarts\", wr=0.1 (Periodically \"restarts\" the LR back to a higher value. Can help escape local minima. Best for: Longer training runs or when stuck in plateaus)  \n",
    "**Linear Decay with Warmup:** lrst=\"linear\", wr=0.05 (LR increases linearly for 5% of training, then decreases linearly to 0)  \n",
    "**Constant with Warmup:** lrst=\"constant_with_warmup\", warmup_steps=100 (LR increases during warmup (for first 100 steps), then stays constant. Best for:** When you've already tuned your LR and it works well)  \n",
    "\n",
    "\n",
    "- bs\n",
    "\n",
    "bs = (rule b**2) 8→16→32→64→128→256 (find the one that fits my GPU/CPU)\n",
    "for fine-tuning small sizes recommended, and smaller sizes work better with small datasets\n",
    "(start with small (e.g., 8) and gradually increase to find the sweet spot between training speed and convergence\n",
    "\n",
    "- epochs\n",
    "\n",
    "epochs = 4-10? 10-15? 20-40? 50?\n",
    "no need to manually tune epochs - monitor validation acc and stop when it doesn't improve for a number of epochs\n",
    "Strategies are random:\n",
    "Starting point: 20-50 epochs\n",
    "Early stopping with patience of 10-15 epochs\n",
    "With around 1000 images per class for binary classification, 10 epochs allow to reach 99% validation acc using ResNet50\n",
    "4 to run quickly\n",
    "\n",
    "- wd\n",
    "\n",
    "wd = 5e-2=0.05 / 1e-2=0.01 / 1e-3=0.001 / 1e-4=0.0001 / 1e-5=0.00001\n",
    "(start with small values (e.g., 1e-4 or 1e-3) and increase if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1793f00-87f0-49a3-8809-522d38427bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "lrst = \"cosine\"\n",
    "wr = 0.15\n",
    "bs = 16\n",
    "epochs = 10\n",
    "wd = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d445c05a-0d88-4557-bdc8-08930a4c3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resnet-50-finetuned\",\n",
    "    remove_unused_columns=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    #fp16=False,  # Disable FP16 to use fp32\n",
    "    #bf16=False,  # Disable BF16 to use fp32\n",
    "    learning_rate=lr,  # lr\n",
    "    per_device_train_batch_size=bs, # bs\n",
    "    per_device_eval_batch_size=bs*2, # bs\n",
    "    num_train_epochs=epochs, # epochs\n",
    "    weight_decay=wd, # wd\n",
    "    #lr_scheduler_type=lrst,  # lrst\n",
    "    #warmup_ratio=wr,  # wr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42cf18-20be-400f-af41-d08bc8e4ca26",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33c4477e-eece-4ffa-aa27-8671f56da1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dc0d1a8-2f7a-4e90-841a-592acd8a4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    processing_class=image_processor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142db8c-6fc0-46fe-b17d-42319eef5eeb",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b767d7a4-2e4b-4f1d-a29e-2b65eddbbd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/800 05:35 < 07:36, 1.01 it/s, Epoch 4.24/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.725689</td>\n",
       "      <td>0.556250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.571624</td>\n",
       "      <td>0.681250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.573049</td>\n",
       "      <td>0.709375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.572245</td>\n",
       "      <td>0.715625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hela/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Runs N epochs of training\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Updates model weights via AdamW optimizer (default)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Evaluates accuracy after each epoch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Saves checkpoints to ./resnet-50-finetuned/\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Result: A fine-tuned model with saved weights and training logs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/transformers/trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/transformers/trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/accelerate/data_loader.py:579\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m next_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2871\u001b[39m, in \u001b[36mDataset.__getitems__\u001b[39m\u001b[34m(self, keys)\u001b[39m\n\u001b[32m   2869\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: \u001b[38;5;28mlist\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m   2870\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2871\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2872\u001b[39m     n_examples = \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[32m   2873\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch.items()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2867\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2865\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2866\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2849\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2847\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2848\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2849\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2851\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:658\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    656\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:415\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:471\u001b[39m, in \u001b[36mPythonFormatter.format_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lazy:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_batch(batch)\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/pata/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:150\u001b[39m, in \u001b[36mPythonArrowExtractor.extract_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Runs N epochs of training\n",
    "# Updates model weights via AdamW optimizer (default)\n",
    "# Evaluates accuracy after each epoch\n",
    "# Saves checkpoints to ./resnet-50-finetuned/\n",
    "# Result: A fine-tuned model with saved weights and training logs\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99a74d-9ee8-4051-bdd8-5262a2959903",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_results = trainer.evaluate()\n",
    "print(f\"\\nStage 1 Results: {stage1_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
